# -*- coding: utf-8 -*-
"""LDA_NEW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rk_qMOnl4Ck6CTiN1RUIvawpVOHJjScd
"""

#imports required
import os
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
import gensim
from gensim.models import LdaModel
from gensim import models, corpora, similarities
import re
from nltk.stem.porter import PorterStemmer
import time
from nltk import FreqDist
from scipy.stats import entropy
import matplotlib.pyplot as plt
#import seaborn as sns
from gensim.test.utils import datapath
#sns.set_style("darkgrid")

#run only once
#nltk.download('stopwords')
#nltk.download('punkt')



def initial_clean(text):
    """
    Function to clean text of websites, email addresess and any punctuation
    We also lower case the text
    """
    text = re.sub("((\S+)?(http(s)?)(\S+))|((\S+)?(www)(\S+))|((\S+)?(\@)(\S+)?)", " ", text)
    text = re.sub("[^a-zA-Z ]", "", text)
    text = text.lower() # lower case the text
    text = nltk.word_tokenize(text)
    return text

stop_words = stopwords.words('english')
def remove_stop_words(text):
    """
    Function that removes all stopwords from text
    """
    return [word for word in text if word not in stop_words]

stemmer = PorterStemmer()
def stem_words(text):
    """
    Function to stem words, so plural and singular are treated the same
    """
    try:
        text = [stemmer.stem(word) for word in text]
        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words
    except IndexError: # the word "oed" broke this, so needed try except
        pass
    return text

def apply_all(text):
    """
    This function applies all the functions above into one
    """
    return stem_words(remove_stop_words(initial_clean(text)))

#clean data

#load data
def load_initial_CSV():
    df = pd.read_csv(os.getcwd()+'/chatbot_tutorial/data/complete_df.csv',
            usecols = ['question_id','tutorial','parsed_title','parsed_body_ques'])
    df = df[df['parsed_body_ques'].map(type) == str]
    df['parsed_body_ques'].fillna(value="", inplace=True)
    df.dropna(axis=0, inplace=True, subset=['parsed_body_ques'])
    print(df.head())

    df['tokenized'] = df['parsed_body_ques'].apply(apply_all) + df['parsed_title'].apply(apply_all)
    
    # only keep articles with more than 2 tokens, otherwise too short
    df = df[df['tokenized'].map(len) >= 2]
    # make sure all tokenized items are lists
    df = df[df['tokenized'].map(type) == list]
    df.reset_index(drop=True,inplace=True)
    
    #get word frequency

    # first get a list of all words
    all_words = [word for item in list(df['tokenized']) for word in item]
    # use nltk fdist to get a frequency distribution of all words
    fdist = FreqDist(all_words)
    len(fdist) # number of unique words
    
    # only keep articles with more than 2 tokens, otherwise too short
    df = df[df['tokenized'].map(len) >= 2]
    # make sure all tokenized items are lists
    df = df[df['tokenized'].map(type) == list]
    df.reset_index(drop=True,inplace=True)
    print(df)
    return df

def train_lda(data):
    
    num_topics = 20
    chunksize = 300
    t1 = time.time()
    dictionary = corpora.Dictionary(data['tokenized'])
    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]  
    
    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,
                   alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=10)
    t2 = time.time()
    
    doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])
    #print("Time to train LDA model on ", len(df), "articles: ", (t2-t1)/60, "min")
    
    return dictionary,corpus,lda,doc_topic_dist

#load data
df = pd.read_csv(os.getcwd()+'/chatbot_tutorial/data/complete_df.csv',
    usecols = ['question_id','tutorial','parsed_title','parsed_body_ques'])
df = df[df['parsed_body_ques'].map(type) == str]
df['parsed_body_ques'].fillna(value="", inplace=True)
df.dropna(axis=0, inplace=True, subset=['parsed_body_ques'])
# shuffle the data
#sdf = df.sample(frac=1.0)
df.reset_index(drop=True,inplace=True)
print(df.head())

def getSavedModel():
    saved_lda_model = datapath("saved_lda_model")
    data=load_initial_CSV()
    
    lda = LdaModel.load(saved_lda_model)
    dictionary = corpora.Dictionary(data['tokenized'])
    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]
    doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])
    
    return dictionary,corpus,lda,doc_topic_dist

def saveLDA_Model(lda):
    saved_lda_model = datapath("saved_lda_model")
    lda.save(saved_lda_model)

def jensen_shannon(query, matrix):
    """
    This function implements a Jensen-Shannon similarity
    between the input query (an LDA topic distribution for a document)
    and the entire corpus of topic distributions.
    It returns an array of length M where M is the number of documents in the corpus
    """
    # lets keep with the p,q notation above
    p = query[None,:].T # take transpose
    q = matrix.T # transpose matrix
    m = 0.5*(p + q)
    return np.sqrt(0.5*(entropy(p,m) + entropy(q,m)))

data=load_initial_CSV()
dictionary,corpus,lda,doc_topic_dist=train_lda(data)

def get_most_similar_documents(query,user_class):
    """
    This function implements the Jensen-Shannon distance above
    and retruns the top k indices of the smallest jensen shannon distances
    """
    query=apply_all(query)
    query_bow = dictionary.doc2bow(query)
    query_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=query_bow)])
    
    sims = jensen_shannon(query_doc_distribution,doc_topic_dist) # list of jensen shannon distances
    most_sim_ids = sims.argsort()[:k]
    most_similar_df = data[data.index.isin(most_sim_ids)]
    return most_similar_df# the top k positional index of the smallest Jensen Shannon distances



query=" downloaded and installed ipython interpreter"

